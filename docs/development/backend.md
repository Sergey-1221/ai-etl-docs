# üêç Backend Development Guide

## Overview

The AI ETL Assistant backend is built with FastAPI, SQLAlchemy 2.0, and async Python patterns for high performance and modern development experience.

## Technology Stack

- **Framework**: FastAPI 0.104+
- **Language**: Python 3.10+
- **Database ORM**: SQLAlchemy 2.0 with async support
- **Database Driver**: asyncpg for PostgreSQL
- **Validation**: Pydantic v2
- **Task Queue**: Celery with Redis
- **Testing**: pytest with async support
- **Code Quality**: Black, Ruff, mypy, Bandit

## Project Structure

```
backend/
‚îú‚îÄ‚îÄ api/                        # API layer
‚îÇ   ‚îú‚îÄ‚îÄ main.py                # FastAPI app initialization
‚îÇ   ‚îú‚îÄ‚îÄ routes/                # API route definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py           # Authentication endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py      # Pipeline management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connectors.py     # Connector configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ observability.py  # Metrics and monitoring
‚îÇ   ‚îî‚îÄ‚îÄ middleware/            # Custom middleware
‚îú‚îÄ‚îÄ core/                      # Core functionality
‚îÇ   ‚îú‚îÄ‚îÄ config.py             # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ database.py           # Database setup
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py       # FastAPI dependencies
‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py         # Custom exceptions
‚îú‚îÄ‚îÄ models/                    # SQLAlchemy models
‚îÇ   ‚îú‚îÄ‚îÄ user.py               # User model
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py           # Pipeline model
‚îÇ   ‚îî‚îÄ‚îÄ connector.py          # Connector model
‚îú‚îÄ‚îÄ schemas/                   # Pydantic schemas
‚îÇ   ‚îú‚îÄ‚îÄ auth.py               # Authentication schemas
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py           # Pipeline schemas
‚îÇ   ‚îî‚îÄ‚îÄ connector.py          # Connector schemas
‚îú‚îÄ‚îÄ services/                  # Business logic layer
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_service.py   # Pipeline operations
‚îÇ   ‚îú‚îÄ‚îÄ auth_service.py       # Authentication logic
‚îÇ   ‚îî‚îÄ‚îÄ llm_service.py        # LLM integration
‚îú‚îÄ‚îÄ validators/                # Data validation
‚îú‚îÄ‚îÄ connectors/               # Data connector implementations
‚îú‚îÄ‚îÄ compliance/               # Compliance modules (GOST, GDPR)
‚îî‚îÄ‚îÄ tests/                    # Test suite
```

## Development Setup

### Prerequisites

```bash
# Install Python 3.10+
python --version  # Should be 3.10 or higher

# Install Poetry for dependency management
pip install poetry

# Install pre-commit hooks
pip install pre-commit
```

### Environment Setup

```bash
# Navigate to backend directory
cd backend

# Create virtual environment and install dependencies
poetry install --with dev

# Activate virtual environment
poetry shell

# Install pre-commit hooks
pre-commit install

# Set up environment variables
cp .env.example .env
# Edit .env with your configuration
```

### Database Setup

```bash
# Start PostgreSQL (using Docker)
docker run -d \
  --name postgres-dev \
  -e POSTGRES_USER=etl_user \
  -e POSTGRES_PASSWORD=etl_password \
  -e POSTGRES_DB=ai_etl_dev \
  -p 5432:5432 \
  postgres:15-alpine

# Run database migrations
alembic upgrade head

# Optional: Seed development data
python -m backend.scripts.seed_dev_data
```

## Core Components

### FastAPI Application Setup

```python
# api/main.py
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse
import structlog

from backend.core.config import get_settings
from backend.core.exceptions import APIError
from backend.api.routes import auth, pipelines, connectors

settings = get_settings()
logger = structlog.get_logger()

app = FastAPI(
    title="AI ETL Assistant API",
    description="AI-powered data pipeline automation",
    version="1.0.0",
    docs_url="/docs" if settings.ENVIRONMENT == "development" else None,
)

# Middleware
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Exception handlers
@app.exception_handler(APIError)
async def api_error_handler(request: Request, exc: APIError):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": {
                "code": exc.error_code,
                "message": exc.message,
                "details": exc.details,
            }
        }
    )

# Health check
@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "ai-etl-backend"}

# Include routers
app.include_router(auth.router, prefix="/api/v1/auth", tags=["auth"])
app.include_router(pipelines.router, prefix="/api/v1/pipelines", tags=["pipelines"])
app.include_router(connectors.router, prefix="/api/v1/connectors", tags=["connectors"])
```

### Configuration Management

```python
# core/config.py
from pydantic_settings import BaseSettings
from typing import List, Optional
from functools import lru_cache

class Settings(BaseSettings):
    # App settings
    APP_NAME: str = "AI ETL Assistant"
    ENVIRONMENT: str = "development"
    DEBUG: bool = False

    # Database
    DATABASE_URL: str
    DATABASE_POOL_SIZE: int = 20
    DATABASE_MAX_OVERFLOW: int = 10

    # Security
    JWT_SECRET_KEY: str
    JWT_ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30

    # CORS
    CORS_ORIGINS: List[str] = ["http://localhost:3000"]

    # LLM Configuration
    OPENAI_API_KEY: Optional[str] = None
    ANTHROPIC_API_KEY: Optional[str] = None

    # External services
    REDIS_URL: str = "redis://localhost:6379"
    AIRFLOW_BASE_URL: str = "http://localhost:8080"

    class Config:
        env_file = ".env"
        case_sensitive = True

@lru_cache()
def get_settings() -> Settings:
    return Settings()
```

### Database Models

```python
# models/base.py
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Column, Integer, DateTime, func
from sqlalchemy.dialects.postgresql import UUID
import uuid

Base = declarative_base()

class BaseModel(Base):
    __abstract__ = True

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
```

```python
# models/pipeline.py
from sqlalchemy import Column, String, Text, Boolean, JSON, ForeignKey
from sqlalchemy.orm import relationship
from sqlalchemy.dialects.postgresql import UUID
from .base import BaseModel

class Pipeline(BaseModel):
    __tablename__ = "pipelines"

    name = Column(String(255), nullable=False)
    description = Column(Text)
    code = Column(Text, nullable=False)
    dag_definition = Column(JSON)
    status = Column(String(50), default="draft")
    is_active = Column(Boolean, default=True)

    # Foreign keys
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"))
    project_id = Column(UUID(as_uuid=True), ForeignKey("projects.id"))

    # Relationships
    user = relationship("User", back_populates="pipelines")
    project = relationship("Project", back_populates="pipelines")
    runs = relationship("Run", back_populates="pipeline")

    def __repr__(self):
        return f"<Pipeline(name='{self.name}', status='{self.status}')>"
```

### Pydantic Schemas

```python
# schemas/pipeline.py
from pydantic import BaseModel, Field, validator
from typing import Optional, Dict, Any, List
from datetime import datetime
import uuid

class PipelineBase(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)
    description: Optional[str] = Field(None, max_length=1000)

class PipelineCreate(PipelineBase):
    code: str = Field(..., min_length=1)
    dag_definition: Optional[Dict[str, Any]] = None

    @validator('name')
    def validate_name(cls, v):
        if not v.replace(' ', '').replace('_', '').replace('-', '').isalnum():
            raise ValueError('Name can only contain letters, numbers, spaces, underscores, and hyphens')
        return v

class PipelineUpdate(BaseModel):
    name: Optional[str] = Field(None, min_length=1, max_length=255)
    description: Optional[str] = Field(None, max_length=1000)
    code: Optional[str] = None
    status: Optional[str] = None

class Pipeline(PipelineBase):
    id: uuid.UUID
    code: str
    status: str
    is_active: bool
    created_at: datetime
    updated_at: Optional[datetime]

    class Config:
        from_attributes = True

class PipelineGenerate(BaseModel):
    description: str = Field(..., min_length=10, max_length=5000)
    project_id: Optional[uuid.UUID] = None
    options: Optional[Dict[str, Any]] = Field(default_factory=dict)

    @validator('description')
    def validate_description(cls, v):
        # Check for potentially dangerous content
        dangerous_keywords = ['drop', 'delete', 'truncate', 'exec', 'eval']
        if any(keyword in v.lower() for keyword in dangerous_keywords):
            raise ValueError('Description contains potentially dangerous keywords')
        return v
```

### Service Layer

```python
# services/pipeline_service.py
from typing import List, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
import structlog

from backend.models.pipeline import Pipeline
from backend.schemas.pipeline import PipelineCreate, PipelineUpdate
from backend.services.llm_service import LLMService
from backend.services.validation_service import ValidationService
from backend.core.exceptions import NotFoundError, ValidationError

logger = structlog.get_logger()

class PipelineService:
    def __init__(
        self,
        db: AsyncSession,
        llm_service: LLMService,
        validation_service: ValidationService
    ):
        self.db = db
        self.llm_service = llm_service
        self.validation_service = validation_service

    async def get_pipelines(
        self,
        user_id: str,
        limit: int = 100,
        offset: int = 0
    ) -> List[Pipeline]:
        """Get user's pipelines with pagination"""
        query = (
            select(Pipeline)
            .where(Pipeline.user_id == user_id)
            .order_by(Pipeline.updated_at.desc())
            .limit(limit)
            .offset(offset)
        )
        result = await self.db.execute(query)
        return result.scalars().all()

    async def get_pipeline(self, pipeline_id: str, user_id: str) -> Pipeline:
        """Get specific pipeline by ID"""
        query = select(Pipeline).where(
            Pipeline.id == pipeline_id,
            Pipeline.user_id == user_id
        )
        result = await self.db.execute(query)
        pipeline = result.scalar_one_or_none()

        if not pipeline:
            raise NotFoundError("Pipeline not found")

        return pipeline

    async def create_pipeline(
        self,
        pipeline_data: PipelineCreate,
        user_id: str
    ) -> Pipeline:
        """Create a new pipeline"""
        logger.info("Creating pipeline", user_id=user_id, name=pipeline_data.name)

        # Validate pipeline code
        validation_result = await self.validation_service.validate_code(
            pipeline_data.code
        )
        if not validation_result.is_valid:
            raise ValidationError(
                "Pipeline code validation failed",
                details=validation_result.errors
            )

        # Create pipeline
        pipeline = Pipeline(
            **pipeline_data.dict(),
            user_id=user_id,
            status="draft"
        )

        self.db.add(pipeline)
        await self.db.commit()
        await self.db.refresh(pipeline)

        logger.info("Pipeline created", pipeline_id=pipeline.id)
        return pipeline

    async def generate_pipeline(
        self,
        description: str,
        user_id: str,
        options: Dict[str, Any] = None
    ) -> Pipeline:
        """Generate pipeline from natural language description"""
        logger.info(
            "Generating pipeline",
            user_id=user_id,
            description_length=len(description)
        )

        try:
            # Generate code using LLM
            generation_result = await self.llm_service.generate_pipeline_code(
                description=description,
                options=options or {}
            )

            # Create pipeline from generated code
            pipeline_data = PipelineCreate(
                name=generation_result.suggested_name,
                description=description,
                code=generation_result.code,
                dag_definition=generation_result.dag_definition
            )

            pipeline = await self.create_pipeline(pipeline_data, user_id)

            logger.info(
                "Pipeline generated successfully",
                pipeline_id=pipeline.id,
                code_lines=len(generation_result.code.splitlines())
            )

            return pipeline

        except Exception as e:
            logger.error("Pipeline generation failed", error=str(e))
            raise

    async def update_pipeline(
        self,
        pipeline_id: str,
        updates: PipelineUpdate,
        user_id: str
    ) -> Pipeline:
        """Update existing pipeline"""
        pipeline = await self.get_pipeline(pipeline_id, user_id)

        # Update fields
        update_data = updates.dict(exclude_unset=True)
        for field, value in update_data.items():
            setattr(pipeline, field, value)

        await self.db.commit()
        await self.db.refresh(pipeline)

        return pipeline

    async def delete_pipeline(self, pipeline_id: str, user_id: str) -> bool:
        """Delete pipeline (soft delete)"""
        pipeline = await self.get_pipeline(pipeline_id, user_id)
        pipeline.is_active = False

        await self.db.commit()
        return True

    async def deploy_pipeline(
        self,
        pipeline_id: str,
        user_id: str,
        environment: str = "production"
    ) -> Dict[str, Any]:
        """Deploy pipeline to Airflow"""
        pipeline = await self.get_pipeline(pipeline_id, user_id)

        # Final validation before deployment
        validation_result = await self.validation_service.validate_for_deployment(
            pipeline.code,
            environment
        )

        if not validation_result.is_valid:
            raise ValidationError(
                "Pipeline failed deployment validation",
                details=validation_result.errors
            )

        # Deploy to Airflow (implementation depends on orchestrator service)
        # This would typically involve:
        # 1. Converting code to Airflow DAG
        # 2. Uploading to Airflow
        # 3. Enabling the DAG

        pipeline.status = "deployed"
        await self.db.commit()

        return {
            "pipeline_id": pipeline.id,
            "status": "deployed",
            "environment": environment,
            "dag_id": f"pipeline_{pipeline.id}"
        }
```

### API Route Implementation

```python
# api/routes/pipelines.py
from fastapi import APIRouter, Depends, HTTPException, status
from typing import List, Optional
import uuid

from backend.schemas.pipeline import (
    Pipeline,
    PipelineCreate,
    PipelineUpdate,
    PipelineGenerate
)
from backend.services.pipeline_service import PipelineService
from backend.core.dependencies import get_current_user, get_pipeline_service
from backend.models.user import User

router = APIRouter()

@router.get("/", response_model=List[Pipeline])
async def get_pipelines(
    limit: int = 100,
    offset: int = 0,
    current_user: User = Depends(get_current_user),
    pipeline_service: PipelineService = Depends(get_pipeline_service)
):
    """Get user's pipelines"""
    pipelines = await pipeline_service.get_pipelines(
        user_id=current_user.id,
        limit=limit,
        offset=offset
    )
    return pipelines

@router.get("/{pipeline_id}", response_model=Pipeline)
async def get_pipeline(
    pipeline_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    pipeline_service: PipelineService = Depends(get_pipeline_service)
):
    """Get specific pipeline"""
    return await pipeline_service.get_pipeline(pipeline_id, current_user.id)

@router.post("/", response_model=Pipeline, status_code=status.HTTP_201_CREATED)
async def create_pipeline(
    pipeline_data: PipelineCreate,
    current_user: User = Depends(get_current_user),
    pipeline_service: PipelineService = Depends(get_pipeline_service)
):
    """Create new pipeline"""
    return await pipeline_service.create_pipeline(pipeline_data, current_user.id)

@router.post(":generate", response_model=Pipeline, status_code=status.HTTP_201_CREATED)
async def generate_pipeline(
    generation_request: PipelineGenerate,
    current_user: User = Depends(get_current_user),
    pipeline_service: PipelineService = Depends(get_pipeline_service)
):
    """Generate pipeline from natural language description"""
    return await pipeline_service.generate_pipeline(
        description=generation_request.description,
        user_id=current_user.id,
        options=generation_request.options
    )

@router.put("/{pipeline_id}", response_model=Pipeline)
async def update_pipeline(
    pipeline_id: uuid.UUID,
    updates: PipelineUpdate,
    current_user: User = Depends(get_current_user),
    pipeline_service: PipelineService = Depends(get_pipeline_service)
):
    """Update pipeline"""
    return await pipeline_service.update_pipeline(
        pipeline_id, updates, current_user.id
    )

@router.post("/{pipeline_id}:deploy")
async def deploy_pipeline(
    pipeline_id: uuid.UUID,
    environment: str = "production",
    current_user: User = Depends(get_current_user),
    pipeline_service: PipelineService = Depends(get_pipeline_service)
):
    """Deploy pipeline to execution environment"""
    return await pipeline_service.deploy_pipeline(
        pipeline_id, current_user.id, environment
    )

@router.delete("/{pipeline_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_pipeline(
    pipeline_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    pipeline_service: PipelineService = Depends(get_pipeline_service)
):
    """Delete pipeline"""
    await pipeline_service.delete_pipeline(pipeline_id, current_user.id)
```

## Testing

### Test Structure

```python
# tests/test_pipeline_service.py
import pytest
from unittest.mock import AsyncMock, Mock
from sqlalchemy.ext.asyncio import AsyncSession

from backend.services.pipeline_service import PipelineService
from backend.schemas.pipeline import PipelineCreate
from backend.models.pipeline import Pipeline

@pytest.fixture
def mock_db():
    return AsyncMock(spec=AsyncSession)

@pytest.fixture
def mock_llm_service():
    return AsyncMock()

@pytest.fixture
def mock_validation_service():
    mock = AsyncMock()
    mock.validate_code.return_value.is_valid = True
    return mock

@pytest.fixture
def pipeline_service(mock_db, mock_llm_service, mock_validation_service):
    return PipelineService(mock_db, mock_llm_service, mock_validation_service)

@pytest.mark.asyncio
async def test_create_pipeline_success(pipeline_service, mock_db):
    """Test successful pipeline creation"""
    # Arrange
    pipeline_data = PipelineCreate(
        name="Test Pipeline",
        description="Test description",
        code="print('test')"
    )
    user_id = "user123"

    # Act
    result = await pipeline_service.create_pipeline(pipeline_data, user_id)

    # Assert
    mock_db.add.assert_called_once()
    mock_db.commit.assert_called_once()
    assert isinstance(result, Pipeline)
    assert result.name == "Test Pipeline"

@pytest.mark.asyncio
async def test_generate_pipeline_success(pipeline_service, mock_llm_service):
    """Test successful pipeline generation"""
    # Arrange
    description = "Load data from PostgreSQL to ClickHouse"
    user_id = "user123"

    generation_result = Mock()
    generation_result.suggested_name = "postgres_to_clickhouse"
    generation_result.code = "# Generated code"
    generation_result.dag_definition = {}

    mock_llm_service.generate_pipeline_code.return_value = generation_result

    # Act
    result = await pipeline_service.generate_pipeline(description, user_id)

    # Assert
    mock_llm_service.generate_pipeline_code.assert_called_once_with(
        description=description,
        options={}
    )
    assert isinstance(result, Pipeline)
```

### Integration Tests

```python
# tests/integration/test_pipeline_api.py
import pytest
from httpx import AsyncClient
from fastapi import status

@pytest.mark.integration
async def test_create_pipeline_endpoint(
    async_client: AsyncClient,
    auth_headers: dict,
    db_session
):
    """Test pipeline creation endpoint"""
    pipeline_data = {
        "name": "Integration Test Pipeline",
        "description": "Test pipeline for integration testing",
        "code": "print('integration test')"
    }

    response = await async_client.post(
        "/api/v1/pipelines/",
        json=pipeline_data,
        headers=auth_headers
    )

    assert response.status_code == status.HTTP_201_CREATED
    data = response.json()
    assert data["name"] == pipeline_data["name"]
    assert "id" in data

@pytest.mark.integration
async def test_generate_pipeline_endpoint(
    async_client: AsyncClient,
    auth_headers: dict
):
    """Test pipeline generation endpoint"""
    generation_request = {
        "description": "Load customer data from PostgreSQL to analytics warehouse",
        "options": {"optimize": True}
    }

    response = await async_client.post(
        "/api/v1/pipelines:generate",
        json=generation_request,
        headers=auth_headers
    )

    assert response.status_code == status.HTTP_201_CREATED
    data = response.json()
    assert "id" in data
    assert "code" in data
    assert len(data["code"]) > 0
```

## Best Practices

### 1. Async/Await Patterns

```python
# Good: Use async/await consistently
async def process_data(data: List[Dict]) -> ProcessedData:
    async with httpx.AsyncClient() as client:
        tasks = [client.get(f"/process/{item['id']}") for item in data]
        responses = await asyncio.gather(*tasks)
    return ProcessedData(responses)

# Avoid: Mixing sync and async
def bad_process_data(data):
    # This blocks the event loop
    response = requests.get("/sync-call")
    return response.json()
```

### 2. Error Handling

```python
# Good: Structured error handling
from backend.core.exceptions import APIError

async def service_method():
    try:
        result = await external_service.call()
        return result
    except ExternalServiceError as e:
        logger.error("External service failed", error=str(e))
        raise APIError(
            status_code=503,
            error_code="EXTERNAL_SERVICE_ERROR",
            message="External service temporarily unavailable",
            details={"service": "external_service", "error": str(e)}
        )
```

### 3. Database Operations

```python
# Good: Use async sessions properly
async def update_user_with_transaction(user_id: str, updates: dict):
    async with get_async_session() as session:
        try:
            user = await session.get(User, user_id)
            if not user:
                raise NotFoundError("User not found")

            for key, value in updates.items():
                setattr(user, key, value)

            await session.commit()
            await session.refresh(user)
            return user
        except Exception:
            await session.rollback()
            raise
```

### 4. Dependency Injection

```python
# Good: Use FastAPI dependency injection
from fastapi import Depends

def get_pipeline_service(
    db: AsyncSession = Depends(get_db),
    llm_service: LLMService = Depends(get_llm_service)
) -> PipelineService:
    return PipelineService(db, llm_service)

@router.post("/pipelines/")
async def create_pipeline(
    service: PipelineService = Depends(get_pipeline_service)
):
    return await service.create_pipeline()
```

## Development Workflow

### 1. Making Changes

```bash
# Create feature branch
git checkout -b feature/new-endpoint

# Make changes with tests
# Run linting and formatting
poetry run black .
poetry run ruff check .
poetry run mypy .

# Run tests
poetry run pytest

# Commit changes
git commit -m "Add new pipeline endpoint"
```

### 2. Code Quality Checks

```bash
# Full quality check
poetry run black --check .
poetry run ruff check .
poetry run mypy .
poetry run bandit -r . -x tests/
poetry run pytest --cov=backend --cov-report=term-missing
```

### 3. Database Migrations

```bash
# Create migration
alembic revision --autogenerate -m "Add new table"

# Review generated migration
# Edit migration file if needed

# Apply migration
alembic upgrade head

# Rollback if needed
alembic downgrade -1
```

## Performance Optimization

### 1. Database Query Optimization

```python
# Good: Eager loading relationships
pipelines = await session.execute(
    select(Pipeline)
    .options(selectinload(Pipeline.runs))
    .where(Pipeline.user_id == user_id)
)

# Good: Batch operations
await session.execute(
    insert(Pipeline).values([
        {"name": "Pipeline 1", "code": "code1"},
        {"name": "Pipeline 2", "code": "code2"},
    ])
)
```

### 2. Caching Strategies

```python
from functools import lru_cache
import redis

# Memory caching for frequently accessed data
@lru_cache(maxsize=1000)
def get_pipeline_template(template_id: str):
    return load_template(template_id)

# Redis caching for shared data
async def get_cached_user_permissions(user_id: str):
    cache_key = f"user_permissions:{user_id}"
    cached = await redis_client.get(cache_key)

    if cached:
        return json.loads(cached)

    permissions = await load_user_permissions(user_id)
    await redis_client.setex(cache_key, 300, json.dumps(permissions))
    return permissions
```

## Security Considerations

### 1. Input Validation

```python
# Always validate and sanitize inputs
@validator('sql_query')
def validate_sql(cls, v):
    # Check for dangerous SQL patterns
    dangerous_patterns = ['drop', 'delete', 'truncate', '--', ';']
    if any(pattern in v.lower() for pattern in dangerous_patterns):
        raise ValueError('SQL query contains dangerous patterns')
    return v
```

### 2. Authorization

```python
# Check user permissions for resources
async def check_pipeline_access(
    pipeline_id: str,
    user_id: str,
    required_permission: str
) -> bool:
    pipeline = await get_pipeline(pipeline_id)
    if not pipeline:
        return False

    # Check ownership
    if pipeline.user_id == user_id:
        return True

    # Check team/project permissions
    return await check_user_permission(
        user_id, pipeline.project_id, required_permission
    )
```

## Related Documentation

- [Development Setup](./setup.md)
- [Frontend Development](./frontend.md)
- [Testing Guide](./testing.md)
- [API Reference](../api/rest-api.md)

---

[‚Üê Back to Development](./README.md) | [Frontend Guide ‚Üí](./frontend.md)